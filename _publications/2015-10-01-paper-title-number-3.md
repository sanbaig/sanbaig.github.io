---
title: "A Hyperconformal Dual-Modal Metaskin for Well-Defined and High-Precision Contextual Interactions"
collection: publications
category: manuscripts
permalink: /publication/2025-06-08-paper-title-number-5
excerpt: 'Shifan Yu, Zhenzhou Ji, Lei Liu, Zijian Huang, <strong><u>Yanhao Luo</u></strong>, Huasen Wang, Ruize Wangyuan, Ziquan Guo, Zhong Chen, Qingliang Liao, Yuanjin Zheng, Xinqin Liao\*'
date: Accepted
venue: 'Nature Communications'
#paperurl: 'http://academicpages.github.io/files/paper3.pdf'
#citation: 'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
---

Proprioception and touch serve as complementary sensory modalities to coordinate hand kinematics and recognize users’ intent for precise interactions. However, current motion-tracking electronics remain bulky and insufficiently precise. Accurately decoding the both is also challenging due to the mechanical crosstalk of endogenous and exogenous deformations. Here, we report a hyperconformal dual-modal (HDM) metaskin for interactive hand motion interpretation. The metaskin integrates a strongly coupled hydrophilic interface with a two-step transfer strategy to minimize interfacial mechanical losses. The 10-μm-scaled hyperconformal film showcases high sensitivity to intricate skin stretches while minimizing signal distortion. It accurately tracks skin stretch as well as touch locations and translates them into polar signals, which are individually salient. This approach enables a differentiable signaling topology within one single data channel without burdening structural complexity to the metaskin. Combining with temporal differential calculation and time-series machine learning network, the metaskin extracts interactive context and action cues from the low-dimensional data. It is further exemplified through demonstrations in contextual navigation, typing and control integration, and multi-scenario object interaction. We demonstrate this fundamental approach in advanced skin-integrated electronics, highlighting its potential for instinctive interaction paradigm and paving the way of augmented somatosensation recognition.
